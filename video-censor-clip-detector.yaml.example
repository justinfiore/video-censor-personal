# Video Censor CLIP Detector Configuration Example
#
# This example demonstrates using OpenAI's CLIP model for efficient,
# configurable content detection. CLIP is lightweight and fast compared
# to vision-language models like LLaVA, making it ideal for resource-
# constrained environments.
#
# Usage:
#   Download models (one-time setup):
#     python -m video_censor --download-models --config video-censor-clip.yaml --input dummy.mp4
#
#   Run analysis:
#     python -m video_censor --config video-censor-clip.yaml --input video.mp4 --output results.json

# Detection settings
detections:
  nudity:
    enabled: true
    sensitivity: 0.7
    model: "local"
  
  violence:
    enabled: true
    sensitivity: 0.6
    model: "local"
  
  sexual_themes:
    enabled: true
    sensitivity: 0.75
    model: "local"

# Detector Configuration
detectors:
  # CLIP-based detector with custom prompts per category
    #
    # Available OpenAI CLIP Models (from Hugging Face):
    # ┌─────────────────────────────────────────┬────────────┬─────────┬─────────────────────────────────────────────────┐
    # │ model_name                              │ Parameters │ Size    │ Notes                                           │
    # ├─────────────────────────────────────────┼────────────┼─────────┼─────────────────────────────────────────────────┤
    # │ openai/clip-vit-base-patch32            │ ~151M      │ ~600MB  │ Fastest, lowest quality. ViT-B/32 architecture. │
    # │ openai/clip-vit-base-patch16            │ ~151M      │ ~600MB  │ Better than patch32, slower. ViT-B/16.          │
    # │ openai/clip-vit-large-patch14           │ ~428M      │ ~1.7GB  │ Best quality for 224px images. ViT-L/14.        │
    # │ openai/clip-vit-large-patch14-336       │ ~428M      │ ~1.7GB  │ Highest quality, fine-tuned for 336px input.    │
    # └─────────────────────────────────────────┴────────────┴─────────┴─────────────────────────────────────────────────┘
    #
    # Trade-offs:
    #   - Patch size: Smaller patches (14 vs 32) = more patches per image = better detail but slower
    #   - Model size: "large" models are ~3x bigger than "base" but significantly more accurate
    #   - The "336" variant expects higher-resolution inputs for maximum accuracy
    #
    - type: "clip"
      name: "clip-detector"
      model_name: "openai/clip-vit-base-patch32"  # Fast & lightweight (see table above for alternatives)
      confidence_threshold: 0.5  # Detection threshold (0.0-1.0); increase for fewer false positives
      categories:
        - "Nudity"
        - "Violence"
        - "Sexual Theme"
      prompts:
        - category: "Nudity"
          text:
            - "nude person"
            - "naked body"
            - "exposed genitals"
            - "private parts"
        
        - category: "Violence"
          text:
            - "fight"
            - "fighting scene"
            - "blood"
            - "injury"
            - "weapon"
            - "gun"
            - "knife"
            - "punch"
            - "hit"
        
        - category: "Sexual Theme"
          text:
            - "sexual activity"
            - "erotic content"
            - "kissing"
            - "intimate scene"
            - "sexual content"

# Audio detection (optional)
audio:
  detection:
    enabled: false

# Video processing settings
processing:
  frame_sampling:
    strategy: "uniform"
    sample_rate: 1.0
  
  segment_merge:
    enabled: true
    merge_threshold: 2.0
  
  max_workers: 4

# Remediation Configuration
remediation:
  # Optional: Audio remediation
  audio:
    enabled: false
    # Audio remediation would be configured here

# Output settings
output:
  format: "json"
  include_confidence: true
  pretty_print: true
